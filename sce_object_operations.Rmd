---
title: "scExploreR Execution Scripts"
output: html_document
date: '2023-02-03'
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(Seurat)
library(SingleCellExperiment)
library(DelayedArray)
library(HDF5Array)
library(DelayedMatrixStats)

library(ggplot2)
library(dplyr)
```

```{r}
# Files used
# Cancer Discovery Seurat object
cancDiscSeurat <- 
  "~/1A_Jordan_Lab/Projects/scExploreR_files/scExploreR/Seurat_Objects/Cancer_discovery_object/pei_cd_20221122.Rds"

# Directory for SingleCellExperiment HDF5 version 
hdf5Dir <-
  "~/1A_Jordan_Lab/Projects/scExploreR_files/scExploreR/Seurat_Objects/Cancer_discovery_object/sce_hdf5"
```

Thursday, February 9th, 2023

The current version of the app is based on Seurat's memory management practices, which require the full matrix to be loaded into memory. This is a considerable barrier to scaling the app to large objects. To address this, usage of the SingleCellExperiment class in place of Seurat will be investigated. The SingleCellExperiment class works with DelayedArray, allowing operations to be performed on the matrix without requiring the full matrix to be loaded into memory.

## Conversion of Seurat object to a SingleCellExperiment

```{r}
# Load a Seurat object
sobj <- readRDS(cancDiscSeurat)

# Convert to SingleCellExperiment object 
# Each assay must be converted separately
sce <- as.SingleCellExperiment(sobj, assay = "RNA")
sceAdt <- as.SingleCellExperiment(sobj, assay = "ADT")

# Store ADT assay as an "alternate experiment" of the RNA assay
altExps(sce) <- list("ADT" = sce_adt)

sce
```

Access Metadata

```{r}
colData(sce) |> names()
```

Access assays (exp. matrices)

```{r}
# List all assays
assays(sce) 

assayNames(sce)

# View scaledata assay
assay(sce, "scaledata")
assays(sce)$scaledata
```

Convert assays to DelayedArrays and combine into a single sce object

```{r}
# No need to do this, see below.
# delArray <- extract_array(assay(sce, "scaledata"), list(NULL, NULL))
# 
# assay(sce, "scaledata") <- delArray
```

Save in HDF5 Format

```{r}
saveHDF5SummarizedExperiment(sce, dir = hdf5Dir, chunkdim = c(1000, 1000))
```

```{r}
Hdf5 <- loadHDF5SummarizedExperiment(dir = hdf5Dir)

print(object.size(Hdf5), units = "auto")
```

Friday, February 10th, 2023

# Operations on SCE objects in HDF5 storage

Expression matrices are stored in the object as delayedMatrix objects. Dplyr functions do not work on delayedMatrix objects, but base R slicing does.

#### Accessing Reductions

Reduction data is not stored as a delayedMatrix (and does not need to be).

```{r}
# View reductions in object
reducedDims(Hdf5)
reducedDimNames(Hdf5)

# Pull UMAP coordinates
reducedDims(Hdf5)[["UMAP"]] |> head()

# Rename reduction columns (will have to do this in app to apply labels chosen 
# in the config app)
new_colnames <-
  reducedDims(Hdf5)[["UMAP"]] |>
  colnames() |>
  (\(.){gsub("UMAP", "Umap", .)})()

colnames(reducedDims(Hdf5)[["UMAP"]]) <- new_colnames

reducedDims(Hdf5)[["UMAP"]] |> head()

# See what happens if an assay is duplicated with a different key
reducedDims(Hdf5)[["Umap"]] <- reducedDims(Hdf5)[["UMAP"]]
head(reducedDims(Hdf5)[["Umap"]])
# Column names do not change when changing the names of dimensionality reductions
```

#### Subsetting

```{r}
# Subset for HSCs and Monocytes
Hdf5[, Hdf5$named_clusters %in% c("HSC", "Monocytes")]

# Fetching genes
assay(Hdf5["HOXA9",], "scaledata")

# Fetching ADTS
# Show all ADTs
assay(altExps(Hdf5)[["ADT"]]) |> rownames()

# summary statistics 
assay(altExps(Hdf5)[["ADT"]][c("CD45", "CD11b"),], "logcounts") |> 
  # Features must be column names (regardless of how many are entered)
  t() |> 
  summary()
```

```{r}
# Complex subsetting
subset <- Hdf5[, (Hdf5$named_clusters %in% c("HSC", "Monocytes") & 
          Hdf5$sample %in% c("Dx"))]

# Inspect "tree" of delayed operations on delayedMatrix elements
subMat <- assay(subset, "counts")
showtree(subMat)
```

The list of delayed operations printed by showtree() is only performed when the new object is "realized" (when the data is printed or loaded into memory by a function). The operations are based on the seed object on disk (instead of duplicating the subset on disk).

#### Other statistics

The operations below are based on the [DelayedArray framework tutorial](https://www.youtube.com/watch?v=Ew_3RdtszBs).

```{r}
# Proportion of nonzero reads in object
mat <- assay(Hdf5, "counts")

# Total number of columns (cells) where each gene has an expression value 
# greater than zero, divided by the number of columns in the object
prop_nonzero <- DelayedArray::rowSums(mat > 0)/ncol(mat)

summary(prop_nonzero)
```

### Block Processing

```{r}
# Temporarially enable verbose block processing
DelayedArray:::set_verbose_block_processing(TRUE)

# Block processing is observed when computing nonzero proportions
DelayedArray::rowSums(mat > 0)/ncol(mat)
```

#### Writing custom functions with block processing

```{r}
basic_colSums <- function(mat){
  # 1. Set up ArrayGrid for block processing
  # Column-wise grids of one column per block are implemented below
  grid <- 
    colAutoGrid(
      mat, 
      ncol = 1
      )
  
  # 2. Load blocks into memory and compute block-level stats
  block_level_stat <- 
    blockApply(
      mat , 
      # Function to apply to each block
      FUN = colSums,
      # Block structure from 1.
      grid = grid
      )
  
  # 3. combine block-level statistics 
  # statistics are returned as a vector using block_level stats
  unlist(block_level_stat)
}

test_value <- basic_colSums(subMat)
```

Each column (cell) becomes a block in this case (there are 2,283 cells). While the RAM requirements are greatly decreased using a block size this small, the function takes a *very* long time to complete.

Below, the block size is increased to 200 cells to increase the computation speed.

```{r}
basic_colSums <- function(mat){
  # 1. Set up ArrayGrid for block processing
  grid <- 
    colAutoGrid(
      mat, 
      # Increase # of columns per block to 200
      ncol = 200
      )
  
  # 2. Load blocks into memory and compute block-level stats
  block_level_stat <- 
    blockApply(
      mat , 
      # Function to apply to each block
      FUN = colSums,
      # Block structure from 1.
      grid = grid
      )
  
  # 3. combine block-level statistics 
  # statistics are returned as a vector using block_level stats
  unlist(block_level_stat)
}

test_value <- basic_colSums(subMat)
```

12 blocks of 200 cells x 32808 features are created in this case, making computation much faster.

```{r}
# Comparison with DelayedMatrix::colSums function
all(DelayedArray::colSums(subMat) == test_value)
```

#### Realizing data on memory or to disk

```{r}
# Realizing in memory
# Can realize as an array or a sparse matrix
fullMat <- as(mat, "sparseMatrix")

# Realizing to disk
# Block processing is used to avoid loading the entire dataset into memory
matrixOnDisk <- writeHDF5Array(mat)
# Can specify a filename, or NULL. When NULL, the matrix is written to the 
# current "HDF5 dump file"

# An object is created in the global environment, but it is a pointer to an 
# HDF5 file, and uses very little memory
print(object.size(matrixOnDisk), units = "auto")
```

Intermediate processing stages are recommended to be saved to disk to avoid requiring re-running the entire tree of delayedMatrix operations to make a plot. [see 42:00 from the DelayedArray Data](https://www.youtube.com/watch?v=Ew_3RdtszBs)

The plotting functions should use a finalized SummarizedExperiment object saved to disk. This is likely to be the case anyway though, based on the current usage of Seurat objects.

#### Changing the defualt block size

The default block size is 100 Mb. This can be changed with `setAutoBlockSize()`.

```{r}
# Block size is given in bytes. The size is set to 500 Mb to increase speed
setAutoBlockSize(size = 5e+08)
```

